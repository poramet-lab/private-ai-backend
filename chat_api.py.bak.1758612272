from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
import httpx, os

router = APIRouter(prefix="/chat", tags=["chat"])

OLLAMA_GEN = "http://127.0.0.1:11435/api/generate"
OLLAMA_EMB = "http://127.0.0.1:11435/api/embeddings"
OPENAI_URL = "https://api.openai.com/v1/chat/completions"
QDRANT_URL = "http://127.0.0.1:6333"
COLLECTION = "demo_rag"

class Message(BaseModel):
    role: str
    content: str

class Controls(BaseModel):
    model_selection: str = Field("chatgpt", description="'chatgpt' | 'local'")
    model_name: str = Field("gpt-4o-mini")
    temperature: float = 0.2
    top_p: float = 0.9
    max_tokens: int = 512
    language: str = "th"
    # เพิ่มสำหรับ auto re-augment
    auto_reaugment: bool = True
    room_scope: str = Field("project", description="'room' หรือ 'project'")
    max_extra_k: int = Field(3, ge=0, le=10)
    score_threshold: float = 0.30

class Packet(BaseModel):
    question: str
    recent_window: list[Message] = []
    rag_bundle: str = ""
    controls: Controls
    room_id: str | None = None  # ใช้ตอน room_scope="room"

class GenResp(BaseModel):
    provider: str
    used_model: str
    answer: str
    sources: list[dict] = []

def build_prompt(p: Packet, ctx: str) -> str:
    recent = "\n".join(f"{m.role}: {m.content}" for m in p.recent_window)
    return (
        f"คุณคือผู้ช่วยทีมพัฒนาซอฟต์แวร์ ตอบเป็นภาษาไทยเท่านั้น และตอบแบบ bullet สั้น กระชับ ไม่เกิน 8 บรรทัด\n"
        f'ห้ามใส่ข้อมูลนอกเหนือจากบริบท ถ้าไม่พอให้ตอบว่า "ข้อมูลไม่พอ"\n\n'
        f"[บริบทจาก RAG]\n{ctx}\n\n"
        f"[บริบทล่าสุด]\n{recent}\n\n"
        f"[คำถาม]\n{p.question}"
    )

def seems_insufficient(text: str) -> bool:
    txt = (text or "").strip()
    if len(txt) < 20:  # สั้นเกินไป
        return True
    for kw in ("ข้อมูลไม่พอ", "ไม่พบข้อมูล", "ไม่มีข้อมูลพอ", "ไม่พอ"):
        if kw in txt:
            return True
    return False

async def call_local_model(client: httpx.AsyncClient, model: str, prompt: str, temperature: float, top_p: float, max_tokens: int) -> str:
    r = await client.post(OLLAMA_GEN, json={
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": temperature, "top_p": top_p, "num_predict": max_tokens}
    })
    if r.status_code != 200:
        raise HTTPException(500, f"Ollama error: {r.text}")
    return r.json().get("response","").strip()

async def call_chatgpt(client: httpx.AsyncClient, model: str, prompt: str, temperature: float, max_tokens: int) -> str:
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        raise HTTPException(400, "Missing OPENAI_API_KEY")
    r = await client.post(OPENAI_URL, json={
        "model": model,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "messages": [
            {"role":"system","content":"ตอบเป็นภาษาไทยเท่านั้น แบบ bullet สั้น กระชับ"},
            {"role":"user","content": prompt}
        ]
    }, headers={"Authorization": f"Bearer {key}"})
    if r.status_code != 200:
        raise HTTPException(500, f"OpenAI error: {r.text}")
    return r.json()["choices"][0]["message"]["content"].strip()

async def embed_query(client: httpx.AsyncClient, text: str) -> list[float]:
    r = await client.post(OLLAMA_EMB, json={"model":"bge-m3","prompt": text})
    if r.status_code != 200:
        raise HTTPException(500, f"Ollama embeddings error: {r.text}")
    emb = r.json().get("embedding")
    if not emb:
        raise HTTPException(500, "No embedding returned")
    return emb

async def search_qdrant(client: httpx.AsyncClient, emb: list[float], limit: int, score_threshold: float, room_id: str | None, scope: str):
    search = {
        "vector": emb,
        "limit": limit,
        "with_payload": True,
        "with_vectors": False,
        "score_threshold": score_threshold
    }
    if scope == "room" and room_id:
        search["filter"] = {"must":[{"key":"room_id","match":{"value":room_id}}]}
    r = await client.post(f"{QDRANT_URL}/collections/{COLLECTION}/points/search", json=search)
    if r.status_code != 200:
        raise HTTPException(500, f"Qdrant search error: {r.text}")
    return r.json().get("result", [])

@router.post("/generate", response_model=GenResp)
async def generate(p: Packet):
    # รอบแรก: ใช้ rag_bundle ที่ส่งมา
    async with httpx.AsyncClient(timeout=120.0) as client:
        prompt = build_prompt(p, p.rag_bundle)
        if p.controls.model_selection == "local":
            ans = await call_local_model(client, p.controls.model_name, prompt, p.controls.temperature, p.controls.top_p, p.controls.max_tokens)
            provider = "local"
        elif p.controls.model_selection == "chatgpt":
            ans = await call_chatgpt(client, p.controls.model_name, prompt, p.controls.temperature, p.controls.max_tokens)
            provider = "chatgpt"
        else:
            raise HTTPException(400, "Unknown model_selection")

        added = []
        if p.controls.auto_reaugment and seems_insufficient(ans) and p.controls.max_extra_k > 0:
            # ฝังเวกเตอร์ question แล้วค้นเพิ่ม
            emb = await embed_query(client, p.question)
            results = await search_qdrant(
                client, emb,
                limit=max(3, p.controls.max_extra_k),
                score_threshold=p.controls.score_threshold,
                room_id=p.room_id,
                scope=p.controls.room_scope
            )
            # กรองรายการใหม่ที่ยังไม่อยู่ใน bundle เบื้องต้น (เช็คชื่อไฟล์ง่าย ๆ)
            base = p.rag_bundle
            appended = 0
            for pt in results:
                pl = pt.get("payload") or {}
                fp = (pl.get("file_path") or "")
                fname = fp.split("/")[-1] if fp else ""
                prev = (pl.get("preview") or "").strip()
                if fname and (fname in base):
                    continue
                added.append({
                    "id": pt.get("id"),
                    "score": round(float(pt.get("score", 0.0)),3),
                    "room": pl.get("room_id"),
                    "file": fname
                })
                base += f"\n\n--- [AUTO-ADD] id={pt.get('id')} room={pl.get('room_id')} file={fp}\n{prev[:1200]}"
                appended += 1
                if appended >= p.controls.max_extra_k:
                    break
            # ถ้ามีเพิ่ม ลองตอบใหม่
            if appended > 0:
                prompt2 = build_prompt(p, base)
                if provider == "local":
                    ans = await call_local_model(client, p.controls.model_name, prompt2, p.controls.temperature, p.controls.top_p, p.controls.max_tokens)
                else:
                    ans = await call_chatgpt(client, p.controls.model_name, prompt2, p.controls.temperature, p.controls.max_tokens)

        return GenResp(provider=provider, used_model=p.controls.model_name, answer=ans, sources=added)
